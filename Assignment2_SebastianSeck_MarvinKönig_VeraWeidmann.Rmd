---
title: "Econometrics BSEL ST 2017 Assignment 2"
author: "Vera Weidmann, Marvin Koenig, Sebastian Seck"
date: "submission to Prof. Qari"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Initialization, warning=FALSE, message=FALSE}
library(foreign) #to read in the .dta format
library(stargazer)
library(dplyr)
library(ggplot2)
library(multcomp)
library(plm)
ksubs <- read.dta("401ksubs.dta")
wagepan <- read.dta("WAGEPAN.DTA")
```

##Task 1
```{r}
head(ksubs)
```

First of all, as the exercise description requires just the data for married couples without children (marr=1, fsize = 2), we have to filter our data. This is done by using the r-package dplyr. 

The new data table is saved in the new data frame: ksubs_1
```{r}
ksubs_1 <- ksubs %>%
  filter(marr ==1 & fsize == 2)
```

#### 1(a) Youngest age of the household head
```{r}
summary(ksubs_1$age)
```
The youngest age of the household head in this sample is 25. 

```{r}
ksubs_1_age <- ksubs_1 %>%
  group_by(age) %>%
  summarise(n=n()) %>%
  arrange(age)

head(ksubs_1_age)
paste("Number of household heads at the age of", ksubs_1_age[1,1],":", ksubs_1_age[1,2])
paste("Number of all households in the restricted dataset:", nrow(ksubs_1))

ggplot(ksubs_1_age,aes(age,n))+geom_col()+labs(
      y="Number of household heads",
      x="Age",
      title="Household heads per age")
```
<br/>
So, 29 households heads are at the age of 25. In total, we have 1494 married households without children. 

#### 1(b) Model & Coefficients (1)
the regression $nettfa = \beta_0 + \beta_1 * inc + \beta_2 * age + \beta_3 * age^2 + u$ gives information about the relationship between the independet variables, also called predictor, ($inc, age, age^2$) and the dependent variable ($nettfa$)

As this model is a level-level model, one unit increase of one of the predictors (while holding the others fix) will lead to an increase in nettfa by the predictors coefficient.

$\frac{\delta nettfa}{\delta inc}=\beta_1$
<br/>
$\frac{\delta nettfa}{\delta age}=\beta_2 + 2*\beta_3*age$

$\beta_0$ is the intercept of the model. This illustarted the value of nettfa when all predictors are equal to 0. 

#### 1(c) Model & Coefficients (2)
```{r}
mod1c <- lm(nettfa ~ inc + age + agesq, data = ksubs_1)
summary(mod1c)

paste("n:", nrow(ksubs_1))
```

As mentioned in 2(b) the intercept of the model with the value of -39.74 is the value of nettfam while predictors are equal to 0. If one predictor is increased by one, e.g. one unit (1,000 dollars) more in inc, that nettfa will increase by $\beta = 1.3$ (1,300 dollars). The increase of one change in age depends on the value of age itself as the regression model included $age^2$. 

The calculation of the turnaround point is 
$x = | \frac{\beta_2}{2*\beta_3} |= |\frac{-1.56905}{2*0.03647}| = 21.51$. This means starting with an age of 22 the nettfam will increase, before 22 the nettfam will decrease. 

The partial effect of age on nettfa for someone who is aged 30 years is:
$$
\frac{\delta nettfa}{\delta age}=\beta_2 + 2*\beta_3*age = -1.56905 + 2*0.03647*30 = 0.61915 = 619\space Dollars 
$$

#### 1(d) Regression for age of 25

$nettfa = \beta_0 + \beta_1 * inc + \beta_2 * age + \beta_3 * age^2 + u$

$\frac{\delta nettfa}{\delta age}=\beta_2 + 2*\beta_3*age$ ; (age=25)

$\theta_2 = \beta_2 + \beta_3 *2* 25$  
$\Leftrightarrow \beta_2 = \theta_2 - \beta_3 *2*25$

Insert in equation from 1(b):  

$nettfa = \beta_0 + \beta_1 * inc + (\theta_2 - \beta_3 *2*25) * age + \beta_3 * age^2 + u$

$nettfa = \beta_0 + \beta_1 * inc + \theta_2 * age - \beta_3 *2*25 * age + \beta_3 * age^2 + u$

$nettfa = \beta_0 + \beta_1 * inc + \theta_2 * age + \beta_3 (age^2 - 2*25*age) + u$

The intercept of the regression model for household heads with age 25 is shifted by the effect of $25^2 = 625$). Therefore, we adjust $\beta_0$ to $\alpha_0$.

$nettfa = \alpha_0 + \beta_1 * inc + \theta_2 * age + \beta_3 (age^2-2*25*age+625) + u$

$nettfa = \alpha_0 + \beta_1 * inc + \theta_2 * age + \beta_3 (age-25)^2 + u$

```{r}
ksubs_1$age25 <- (ksubs_1$age-25)^2

mod1d <- lm(nettfa ~ inc + age + age25, data = ksubs_1)
(sum_mod1d <- summary(mod1d))
```

$H_0: \theta_2 =0$ ; (two-sided test, $\alpha = 0.05$)

As the p-value of $\theta_2 = 0.72294 > 0.05$, $H_0$ cannot be rejected. This variable might not have an significant effect on the dependet variable nettfam. 

The lincom command (in R: car-package) can be used to model the equation by using the original one without modifing the model. 



HELP!!!


lincom command??? Linear combinations of parameters -> linear hypothesis- package: car? john foxed
K?nnen es auch weg lassen nach Quari :D 


```{r}
library(multcomp)
summary(glht(mod1c, linfct = c("age = 25")))

#library(gmodels)
library(car)
linearHypothesis(mod1c, "age = 25") #?????????
```










#### 1(e) Differences between $\beta_0$ and $\alpha_0$
```{r}
paste("the difference between the point estimates are:", round(mod1c$coefficients[1] - mod1d$coefficients[1],2))
```

In the first model (mod1b) the intercept presents the value of nettfa while all independet variables (inc and age) are equal to 0. As the age of one household head cannot be 0, the second equation (mod1d) makes more sense. By using (age-25) we are centering the all variables around an age of 25 years. In that case, also the intercept is shifted by -22.79. So, it shows the effect of being 25 years old while other variables are equal to 0. 

#### 1(f) The effect of age variable
```{r}
mod1f <- lm(nettfa ~ inc + age25, data = ksubs_1)
(sum_mod1f<- summary(mod1f))
```

```{r, results="asis"}
stargazer(mod1d,mod1f, title = "Comparison of models mod1f and mod1d", type="html", model.names = FALSE,column.labels = c("mod1f", "mod1d"), column.separate = c(1,1), style = "qje")
```

```{r}
paste("adjusted R_squared of Model 1d:", sum_mod1d$adj.r.squared)
paste("adjusted R_squared of Model 1f:", sum_mod1f$adj.r.squared)
```

Looking at the adjusted $R^2$, it is seen that both models have a quite similar model quality. In both models nearly 20.3% of nettfa variance can be explanied by the predictors. This measurement supports the low significance level of age in model 1d. As mentioned in 1d, $H_0: \space \theta_2 =0$ cannot be rejected. Therefore, the variable age can be omitted without losing model quality. 

#### 1(g) Income of 50,000 

```{r}
paste("The estimated  nettfa of an household which has an income of 50000 is:", round(sum_mod1f$coefficients[1,1] + sum_mod1f$coefficients[2,1] * 50 *1000,2))
```
```{r}
ksubs_1$preds <- predict(inc=50, mod1f)

ggplot(ksubs_1,aes(x=age,y=preds))+geom_point()+geom_abline(col="red", size=0.8)+labs(
      x="Age of household heads",
      y="Prediction of nettfa",
      title="Relationship between nettfa and age",
      subtitle= "while holding income = $50,000 fix")
```

The plot shows a slight increase of nettfa by increasing the age of the household head. This is also seen in the model of 1c and 1d, which show the slope of income of $\beta_1 = 1.333$ ($1333).

##Task 2 Heteroscedasticity
For task 2, we have to restrict our data to single individuals (fsize =1).
The new data table is saved in the new data frame: ksubs_2
```{r}
ksubs_2 <- ksubs %>%
  filter(fsize == 1)
```

#### 2(a) Test heteroscedasticity
```{r}
ksubs_2$age25 <- (ksubs_2$age-25)^2
mod2a <- lm(nettfa ~ inc + age25 + e401k, data = ksubs_2)
#summary(mod2a)
```

To check whether our data is underlying a heteroscedastic distribution, we test the residuals of the previous model. This can be made by running a regression on the squared error residuals:

$residuals^2 = \delta_0 + \delta_1 * inc + \delta_2 * (age-25) * \delta_3 *e401k + u$

```{r}
ksubs_2$residuals <- mod2a$residuals^2
mod2a_resid <- lm(residuals ~ inc + age25 + e401k, data = ksubs_2)
(sum_mod2a_resid <- summary(mod2a_resid))
```

The null hypothesis tests if there is no difference in the variance of the residuals. So $H_0$ implies that our data is homoscedastically distributed (variance of 0). 

$H_0:\delta_1 =0, \delta_2 =0, \delta_3 =0$ ; ($\alpha = 0.05$, two-sided) <br/>
$H_1:\delta_1 \neq0, \delta_2 \neq0, \delta_3 \neq0$

Rejection rule: F > critical value

```{r}
paste("The f-statistic of model mod2a_resid is:", round(sum_mod2a_resid$fstatistic[1],2))
```

The critical value, which can be found in the f-table (on moodle) is 2.61 (using $k=3, \alpha = 0.05, df = 2013 $). This value can be also calculated via r:
```{r}
#(1-/alpha, k, n-k-1)
qf(0.95, 3, nrow(ksubs_2)-4)
```
As the calculated f-value of 4.48 is greater than the critical value of 2.61, we reject $H_0$. This means we have heteroscedasticity. 

This can be also tested by the LM version:

$LM = n * R^2_u$

```{r}
LM = nrow(ksubs_2) * sum_mod2a_resid$r.squared
paste("The LM value is:", round(LM,2))
```
As this value is far greater than the critical value of 2.61, we can reject $H_0$.

####2(b) WLS model
Due to the White Least Squares Estiation (WLS) Method data which underlyies heteroscedasticity can be transformed to a homoscedastic distibution. We assume $Var(u_i|inc_i) = \sigma^2$. Therefore, we divide all variables by the squareroot of income. 

Heteroscedastic model:

$$
nettfa= \beta_0 + \beta_1 * inc + \beta_2 * (age-25)^2 + \beta_3 * e401k + u
$$

Transformed/ homosceadstic model:

$$
\frac{nettfa}{\sqrt{inc}}= \beta_0 * \frac{1}{\sqrt{inc}}+ \beta_1 * \frac{inc}{\sqrt{inc}} + \beta_2 * \frac{(age-25)^2}{\sqrt{inc}} + \beta_3 * \frac{e401k}{\sqrt{inc}} + u
$$

Manually, we could create new variables by calculating each used variable divided by $\sqrt{inc}$. In addition, we have to adjust $\beta_0$ by $\frac{1}{\sqrt{inc}}$.

However, to run a WLS model in R, we just have to assign the weight to the regression command. In this case the weight is $h = inc. 
```{r}
mod2b <- lm(nettfa ~ inc + age25 + e401k, data = ksubs_2, weights = 1/ksubs_2$inc)
summary(mod2b)
```
```{r}
#manual approach: 
ksubs_2$h <- 1/sqrt(ksubs_2$inc) #weight
ksubs_2$nettfa_h <- ksubs_2$nettfa*ksubs_2$h
ksubs_2$inc_h <- ksubs_2$inc*ksubs_2$h
ksubs_2$age25_h <- ksubs_2$age25*ksubs_2$h
ksubs_2$e401k_h <- ksubs_2$e401k*ksubs_2$h
ksubs_2$intercept_h <- ksubs_2$h

mod2b_manually <- lm(nettfa_h ~ inc_h + age25_h + e401k_h + intercept_h -1, data = ksubs_2)
summary(mod2b_manually)
```


Comparing models 2a and 2b: 
```{r,results="asis"}
stargazer(mod2a,mod2b, mod2b_manually, title = "Comparison of models mod2a and mod2b", type="html", model.names = FALSE,column.labels = c("mod2a", "mod2b", "mod2b manually"), column.separate = c(1,1), style = "qje")
```
It can be seen that the coeffiecients are smaller in the WLS model than in the normal OLS model. Furthermore, the Root-MSE is quite smaller than before. 

However, comparing the manually calculated model in 2b with the one in R, we gain a a different value for R^2. 

####2(c) GLS 

Under assuming  $Var(u_i|inc_i) = \sigma^2 exp(\delta_0+\delta_1inc + \delta(age-25)^2+\delta_3e401k$, we use a generalized least squares (GLS) apporach. This is done by running a regression on the log(residuals^2) of model 2a: 
$log(u^2) = exp(\delta_0+\delta_1inc + \delta(age-25)^2+\delta_3e401k$

Afterwards, we calcualte h by using the exp-function on the model output.
$h= exp(\delta_0+\delta_1inc + \delta(age-25)^2+\delta_3e401k$

```{r}
ksubs_2$logresiduals <- log(ksubs_2$residuals)
```
```{r}
mod2c_resid <- lm(logresiduals ~ inc + age25 + e401k, data = ksubs_2)
ksubs_2$h <- exp(predict(mod2c_resid))

mod2c_gls <- lm(nettfa ~ inc + age25 + e401k, data = ksubs_2, weights = 1/ksubs_2$h)
summary(mod2c_gls)
```

##Task 3 Panel Data
-> Chapter 14!! Slight different to what we did in class today :/

####3(a) Pooled OLS
```{r}
mod3a <- lm(lwage ~ educ + black + hisp + exper + expersq + married + union, data = wagepan)
summary(mod3a)
```


####3(b) Random Effects
```{r, warning=FALSE}
pwagepan = plm.data(wagepan, index=c("nr","year"))

mod3b <- plm(lwage ~ educ + black + hisp + exper + expersq + married + union, data = pwagepan, model = "random")
summary(mod3b)
```

```{r, results="asis"}
stargazer(mod3a, mod3b, title = "Comparison of models mod3a and mod3b", type = "html", column.labels = c("mod3a","mod3b"), style = "qje")
```
As seen in the table, the difference between the models is not stark. The $R^2$ is close to each other, further all variables have matching signs, the only larger deviation observed from another is with the intercept.
As we know from the random effects assumtion, the fixed error $a_i$ is uncorrelated with the explanatory variables but is serially correlated with the individuals.

#### 3(c) Comparison of RE and OLS standard errors
Under the random effects assumption, the standard errors of the OLS model are invalid unless $a_i$ can be neglected. Further, they are often invalid since the serial correlation in the composite error of $a_i$ and $u_{it}$ is ignored. RE standard errors take the serial correlation and heteroskedasticity into account, thus they are adjusted for the fact that errors are correlated over time for a given i, which is why they are more reliable.

#### 3(d) Focus on variables changing over time
##### 3(d)1 Fixed effects model
```{r}
means <- aggregate(pwagepan[,c("expersq","married","union")],list(pwagepan$nr), mean)
names(means)
colnames(means) <- c("nr","mean_expersq","mean_married","mean_union")
pwagepan <- merge(pwagepan,means)
pwagepan$FEexpersq <- pwagepan$expersq - pwagepan$mean_expersq
pwagepan$FEmarried <- pwagepan$married - pwagepan$mean_married
pwagepan$FEunion <- pwagepan$union - pwagepan$mean_union
mod3d1 <- lm(lwage ~ FEexpersq + FEmarried + FEunion, data = pwagepan)
summary(mod3d1)

# This is simply to verify the correctness of the above
mod3d1Check <- plm(lwage ~ expersq + married + union, data = pwagepan, model = "within")
summary(mod3d1Check)
```
##### 3(d2) Corrections of DF and SEs
The adjusted number for degrees of freedom is $NT-N-k = 4360-545-3 = 3812$. This is because we adjust for the averagingm which was done on 545 different groups.

```{r}
(mod3d1_R2 <- sum(mod3d1$residuals^2)/3812)
paste("The corrected standard error is",round(mod3d1_R2,3))
```

Corrected standard errors:
```{r}
mod3d2a <- lm(formula = FEexpersq ~ FEmarried + FEunion, data = pwagepan)
summod3d2a <- summary(mod3d2a)
a = sum(mod3d2a$residuals^2)
b = sum(mod3d2a$fitted.values^2)
(SEexpersq <- sqrt(mod3d1_R2/(a+b*(1-summod3d2a$r.squared))))

mod3d2b <- lm(formula = FEmarried ~ FEexpersq + FEunion, data = pwagepan)
summod3d2b <- summary(mod3d2b)
c = sum(mod3d2b$residuals^2)
d = sum(mod3d2b$fitted.values^2)
(SEmarried <- sqrt(mod3d1_R2/(c+d*(1-summod3d2b$r.squared))))

mod3d2c <- lm(formula = FEunion ~ FEexpersq + FEmarried, data = pwagepan)
summod3d2c <- summary(mod3d2c)
e = sum(mod3d2c$residuals^2)
f = sum(mod3d2c$fitted.values^2)
(SEunion <- sqrt(mod3d1_R2/(e+f*(1-summod3d2c$r.squared))))
```

##### 3(d3) Main difference between RE and FE
* For random effect assumption the composite error $a_i + u_{it}$ is uncorrelated with the explanatory variables
but it is serially correlated for observations coming from the same individual
* In the fixed effect model it is assumed that there is a fixed effect $a_i$ which may show correlation with explanatory variables, unlike a random error term.